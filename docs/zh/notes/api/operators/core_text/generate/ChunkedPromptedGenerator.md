---
title: ChunkedPromptedGenerator
createTime: 2026/01/20 15:00:00
permalink: /zh/api/operators/core_text/generate/chunkedpromptedgenerator/
---

## ğŸ“˜ æ¦‚è¿°

`ChunkedPromptedGenerator` æ˜¯ä¸€ä¸ªæ”¯æŒ**é•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—**çš„æç¤ºè¯ç”Ÿæˆç®—å­ã€‚å½“è¾“å…¥å†…å®¹è¶…è¿‡é¢„è®¾çš„ Token é™åˆ¶æ—¶ï¼Œè¯¥ç®—å­ä¼šé‡‡ç”¨é€’å½’äºŒåˆ†æ³•å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªè¾ƒå°çš„å—ï¼ˆChunksï¼‰ï¼Œåˆ†åˆ«è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç»“æœï¼Œæœ€åå°†å„å—çš„ç”Ÿæˆå†…å®¹æŒ‰æŒ‡å®šåˆ†éš”ç¬¦æ‹¼æ¥ã€‚

å®ƒç‰¹åˆ«é€‚ç”¨äºå¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆå¦‚ä¹¦ç±ã€é•¿è®ºæ–‡ï¼‰ï¼Œå¹¶æ”¯æŒä»æ–‡ä»¶è·¯å¾„è¯»å–è¾“å…¥å†…å®¹ã€‚

## `__init__`å‡½æ•°

```python
def __init__(self, 
             llm_serving: LLMServingABC, 
             system_prompt: str = "You are a helpful agent.", 
             json_schema: dict = None, 
             max_chunk_len: int = 128000, 
             enc = tiktoken.get_encoding("cl100k_base"), 
             seperator: str = "\n"
             )
```

### initå‚æ•°è¯´æ˜

| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
| --- | --- | --- | --- |
| **llm_serving** | LLMServingABC | å¿…éœ€ | å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å®ä¾‹ã€‚ |
| **system_prompt** | str | "You are a helpful agent." | ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰æ¨¡å‹çš„è§’è‰²å’Œè¡Œä¸ºã€‚ |
| **json_schema** | dict | None | ï¼ˆå¯é€‰ï¼‰ç”¨äºçº¦æŸ LLM è¾“å‡ºæ ¼å¼çš„ JSON Schemaã€‚ |
| **max_chunk_len** | int | 128000 | å•ä¸ªåˆ†å—çš„æœ€å¤§ Token æ•°é‡ã€‚ |
| **enc** | Encoder/Tokenizer | tiktoken.get_encoding("cl100k_base") | ç”¨äºè®¡ç®— Token çš„ç¼–ç å™¨ï¼Œéœ€æ”¯æŒ `encode` æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥æ˜¯AutoTokenizerã€‚ |
| **seperator** | str | "\n" | å¤šä¸ªåˆ†å—ç”Ÿæˆç»“æœä¹‹é—´çš„æ‹¼æ¥åˆ†éš”ç¬¦ã€‚ |

### åˆ†å—é€»è¾‘è¯´æ˜

è¯¥ç®—å­é‡‡ç”¨**é€’å½’äºŒåˆ†æ³•**ï¼š

1. è®¡ç®—å½“å‰æ–‡æœ¬çš„ Token æ€»æ•°ã€‚
2. è‹¥ Token æ•° < `max_chunk_len`ï¼Œåˆ™ä½œä¸ºä¸€ä¸ªç‹¬ç«‹å—å¤„ç†ã€‚
3. è‹¥ Token æ•° > `max_chunk_len`ï¼Œåˆ™ä»æ–‡æœ¬ä¸­é—´å­—ç¬¦ä½ç½®åˆ‡åˆ†ä¸ºå·¦å³ä¸¤éƒ¨åˆ†ï¼Œé€’å½’è¿›è¡Œä¸Šè¿°åˆ¤æ–­ã€‚

## `run`å‡½æ•°

```python
def run(self, storage: DataFlowStorage, input_path_key: str, output_path_key: str)

```

æ‰§è¡Œç®—å­é€»è¾‘ï¼šä»æŒ‡å®šçš„è¾“å…¥åˆ—ä¸­è¯»å–æ–‡ä»¶è·¯å¾„ï¼ŒåŠ è½½æ–‡ä»¶å†…å®¹ï¼Œåˆ†å—ç”Ÿæˆåå°†ç»“æœå†™å…¥æ–°çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¹¶åœ¨ DataFrame ä¸­è®°å½•è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚

#### å‚æ•°

| åç§° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
| --- | --- | --- | --- |
| **storage** | DataFlowStorage | å¿…éœ€ | æ•°æ®æµå­˜å‚¨å®ä¾‹ã€‚ |
| **input_path_key** | str | å¿…éœ€ | è¾“å…¥åˆ—åï¼Œè¯¥åˆ—åº”åŒ…å«å¾…å¤„ç†æ–‡æœ¬æ–‡ä»¶çš„**æœ¬åœ°è·¯å¾„**ã€‚ |
| **output_path_key** | str | å¿…éœ€ | è¾“å‡ºåˆ—åï¼Œç”¨äºå­˜å‚¨ç”Ÿæˆçš„ LLM è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚ |

## ğŸ§  ç¤ºä¾‹ç”¨æ³•

```python
from dataflow.core import LLMServing
from dataflow.utils.storage import DataFlowStorage

# åˆå§‹åŒ–ç®—å­ï¼Œè®¾ç½®æœ€å¤§é•¿åº¦ä¸º 2000 tokens
operator = ChunkedPromptedGenerator(
    llm_serving=my_llm_instance,
    max_chunk_len=2000,
    seperator="\n---\n"
)

# è¿è¡Œç®—å­
operator.run(
    storage=my_storage,
    input_path_key="file_path",
    output_path_key="result_path"
)

```

#### ğŸ§¾ è¾“å‡ºé€»è¾‘è¯´æ˜

ç®—å­ä¼šè‡ªåŠ¨åœ¨è¾“å…¥æ–‡ä»¶çš„åŒçº§ç›®å½•ä¸‹ç”Ÿæˆåç¼€ä¸º `_llm_output.txt` çš„ç»“æœæ–‡ä»¶ã€‚

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
| --- | --- | --- |
| file_path | str | åŸå§‹è¾“å…¥æ–‡ä»¶çš„è·¯å¾„ï¼ˆä¾‹å¦‚ `data/doc.txt`ï¼‰ã€‚ |
| result_path | str | ç”Ÿæˆç»“æœæ–‡ä»¶çš„å­˜å‚¨è·¯å¾„ï¼ˆä¾‹å¦‚ `data/doc_llm_output.txt`ï¼‰ã€‚ |

**ç¤ºä¾‹è¾“å…¥ DataFrame è¡Œï¼š**

```json
{
  "file_path": "/home/user/data/long_article.txt"
}

```

**åˆ†å—å¤„ç†æµç¨‹ï¼š**

1. è¯»å– `long_article.txt` å†…å®¹ã€‚
2. å‡è®¾æ–‡æœ¬è¢«åˆ‡åˆ†ä¸º Chunk A å’Œ Chunk Bã€‚
3. è°ƒç”¨ LLM å¾—åˆ° `Result A` å’Œ `Result B`ã€‚
4. å°† `Result A\nResult B` å†™å…¥ `/home/user/data/long_article_llm_output.txt`ã€‚

**ç¤ºä¾‹è¾“å‡º DataFrame è¡Œï¼š**

```json
{
  "file_path": "/home/user/data/long_article.txt",
  "result_path": "/home/user/data/long_article_llm_output.txt"
}

```
