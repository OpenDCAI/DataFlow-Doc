---
title: Operator Reuse / Prompt Optimization
createTime: 2026/02/05 22:11:00
permalink: /en/guide/agent/pipeline_prompt/
---

## 1. Overview

**Prompt Optimization** is the core module of DataFlow-Agent designed for **Prompt-Engineering**. Its design goal is to solve the problem of "generic operator logic reuse".

This module adopts a **single-node architecture**. When a user proposes a new data processing requirement, the Agent not only writes a Prompt that complies with operator specifications but also **automatically generates synthetic test data** and internally builds and runs test scripts.


## 2. Core Features

### 2.1 Example-Based Generation

* **Operator Code Analysis**: The Agent automatically reads the source code of the target operator (`OP_NAME`) and extracts its parameter definitions.
* **Prompt Migration**: The system retrieves existing Prompt cases from the operator library and uses them as context to guide the LLM in generating a new Prompt class that conforms to the operator's interface specifications (e.g., `init` parameter structure).

### 2.2 Self-Verification Based on Synthetic Data

The generated Prompt does not merely remain as text; the Agent immediately tests it.

* **Data Synthesis**: The Agent **does not require** large-scale business data from the user for testing. Instead, it utilizes the LLM to analyze the operator logic and automatically generates a set of **synthetic test data** covering various edge cases, saving it as a temporary JSONL file.
* **Subprocess Execution**: The Agent internally and automatically builds a temporary Python test script and launches a subprocess to execute it, verifying whether the generated Prompt runs correctly and produces the expected results.

### 2.3 Iterative Optimization

* **Interactive Feedback**: The system does not perform blind automatic retries. Users input modification suggestions after viewing the test results, the generated Prompt, and data previews on the frontend.
* **Targeted Hot Update**: Upon receiving feedback, the backend `PromptWriter` calls the `revise_with_feedback` method to make targeted modifications to the Prompt code while maintaining the existing context, automatically triggering a new round of the testing loop.

## 3. System Architecture

This function is defined by `wf_pipeline_prompt.py`, featuring a core **single-node** workflow. All generation and verification logic are highly cohesive within the `PromptWriter` Agent.

### 3.1 Core Node Process

The **Prompt Writer Node** is the only node in the graph, executing the following internal logic in sequence:

1. **Context Retrieval**: Calls Pre-tools to obtain the target operator's source code, the user's target, and Prompt examples.
2. **Prompt Generation**: Calls the LLM to generate the Prompt class code in Python form. It then saves the generated code to the `state` object via the `update_state_result` method and writes it to a local file to provide dependencies for subsequent test steps.
3. **Test Data Synthesis**: Calls the internal method `_build_test_data_by_llm` to generate synthetic test data based on the task description.
4. **Test Script Construction**: Calls the internal method `_build_test_code` to generate a temporary test script using string templates.
5. **Subprocess Execution**: Uses `subprocess` to run the test script and captures standard output (stdout) and standard error (stderr).
6. **Test Result Output**: Scans and reads the test result file generated by the subprocess execution, updates the test results into `state.temp_data`, and completes the process.

### 3.2 Iterative Optimization Mechanism

The optimization process depends on **frontend interaction**:

1. The user views the execution results in the UI.
2. The user submits feedback.
3. The frontend calls `_on_chat_submit`, triggering the Agent's `revise_with_feedback` interface.
4. The Agent modifies the code based on the feedback and re-executes the validation phase described above (Test Data Synthesis -> Test Script Construction -> Subprocess Execution).

## 4. User Guide

### 4.1 Graphical Interface

The frontend code is located in `PA_frontend.py`, providing a complete interactive development environment.

**Initial Generation:**

1. Configure API information (URL, Key, Model).
2. Fill in the task description and operator name.
3. (Optional) Specify the output format, argument list, and file output root path.
4. Click the "Generate Prompt Template" button.
5. View the test data, test results, Prompt code, and test code generated by the Agent.

**Multi-turn Optimization:**

1. If the results do not meet expectations, enter improvement suggestions in the dialogue box on the right.
2. Click "Send Rewrite Instruction".
3. View the updated code and test results.
4. Repeat steps 1-3 until a satisfactory result is obtained.

**Using the Generated Prompt:**

1. Get the generated Prompt file location from "Prompt File Path".
2. Import the Prompt class into your operator.
3. Specify `prompt_template` in the operator's `init()`.

### 4.2 Script Invocation

Use the `run_dfa_pipeline_prompt.py` script for automated generation.

**Configuration Parameters:**

```python
CHAT_API_URL = os.getenv("DF_API_URL", "http://123.129.219.111:3000/v1/")
MODEL = os.getenv("DF_MODEL", "gpt-4o")
LANGUAGE = "en"

TASK_DESCRIPTION = "Write a prompt for an operator that filters missing values"
OP_NAME = "PromptedFilter"

# These two items are only required if the operator does not possess any preset prompts; 
# otherwise, it will generate based on existing prompts.
OUTPUT_FORMAT = ""  # e.g., "Return JSON with keys: ..."
ARGUMENTS = []      # e.g., ["min_len=10", "drop_na=true"]

# Cache directory used to store test data and prompts
CACHE_DIR = "./pa_cache"
DELETE_TEST_FILES = True

```

**Run Command:**

```bash
python run_dfa_pipeline_prompt.py

```