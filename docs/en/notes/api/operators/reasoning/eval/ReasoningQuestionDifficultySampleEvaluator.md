---
title: ReasoningQuestionDifficultySampleEvaluator
createTime: 2025/10/09 16:52:48
permalink: /en/api/operators/reasoning/eval/reasoningquestiondifficultysampleevaluator/
---

## ðŸ“˜ Overview
The `ReasoningQuestionDifficultySampleEvaluator` is an operator designed to evaluate the difficulty level of questions. It leverages a Large Language Model (LLM) to analyze the complexity of a given question and outputs a numerical difficulty score, typically on a scale of 1 to 10.

## __init__
```python
def __init__(self, llm_serving: LLMServingABC = None)
```
| Parameter | Type | Default | Description |
| :--- | :--- | :--- | :--- |
| **llm_serving** | LLMServingABC | None | An instance of a large language model serving class, used for executing inference and generation. |

### Prompt Template Descriptions
| Prompt Template Name | Primary Use | Applicable Scenarios | Features |
| :--- | :--- | :--- | :--- |
| **MathQuestionDifficultyPrompt** | | | |

## run
```python
def run(self, storage: DataFlowStorage, input_key: str, output_key: str = "difficulty_score")
```
| Parameter | Type | Default | Description |
| :--- | :--- | :--- | :--- |
| **storage** | DataFlowStorage | Required | An instance of the DataFlow storage, responsible for reading and writing data. |
| **input_key** | str | Required | The name of the input column, corresponding to the question field. |
| **output_key** | str | "difficulty_score" | The name of the output column, corresponding to the generated difficulty score field. |

## ðŸ§  Example Usage

#### ðŸ§¾ Default Output Format
| Field | Type | Description |
| :--- | :--- | :--- |
| ... | ... | ... |
| difficulty_score | float | The numerical difficulty score (1-10) generated by the model. -1 if parsing fails. |
